<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xml:lang="en" lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=UTF-8" http-equiv="Content-Type" />
<meta name="generator" content="http://www.nongnu.org/elyxer/" />
<meta name="create-date" content="2010-05-03" />
<link href="research_notes.css" rel="stylesheet" media="screen" type="text/css" />
<title>Research notes on Vision for mobile robots</title>
</head>
<body>
<div id="globalWrapper">

<h1 class="title">
Research notes on<span class="giant"> <br />
Vision for mobile robots</span>
</h1><iframe scrolling="no" allowtransparency="true" src="http://www.facebook.com/plugins/like.php?href=http%3A%2F%2Frodrigob.github.com&amp;layout=standard&amp;show_faces=true&amp;width=450&amp;action=like&amp;colorscheme=light" frameborder="0" id="facebook_like" style="border:none; overflow:hidden; width:450px; height:2em"></iframe>

<h2 class="author">
Created by <a href="http://www.google.com/profiles/rodrigo.benenson" class="URL">Rodrigo Benenson</a>
</h2>
<h2 class="Date">Last updated on 2010-05-03</h2>
<h2 class="Section-">
<a name="toc-Section*-1" class="toc"></a>What is this document ?
</h2>
<div class="Unindented">
This is a live document where I keep my though on my current research.
</div>
<div class="Indented">
After finishing my PhD thesis <sup><a name="cite-6" href="#biblio-6" class="bibliocite">6</a></sup> on mobile robotics I decided to continue my work by exploring the world of vision. This document is an attempt to gather and share my though and knowledge on the subject.
</div>
<div class="Indented">
Blogs are good, but lack the consistency of a book. Books are good, but cannot be shared as easily as online documents. This is an attempt to share knowledge in a new form.
</div>
<h2 class="Section-">
<a name="toc-Section*-2" class="toc"></a>License
</h2>
<div class="Unindented">

<div class="center">
This document is shared under the<br />
 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/" class="URL">Creative Commons Attribution-Noncommercial-Share Alike 3.0 </a> license.
</div>
</div>
<div class="Indented">
The document is shared in an open format, you are free to copy it, branch it, remix it, as long as you respect the mentioned creative commons license. 
</div>
<div class="Indented">
If you are interested on collaborating with me on this document, please feel free to <a href="http://www.google.com/profiles/rodrigo.benenson" class="URL">contact me</a>.
</div>
<h2 class="Section-">
<a name="toc-Section*-3" class="toc"></a>Acknowledgements
</h2>
<div class="Unindented">
I do my research at <a href="http://kuleuven.be" class="URL">KULeuven</a>, first I woud like to thank <a href="http://www.esat.kuleuven.be/psi/visics" class="URL">Prof. Van Gool</a> for giving me the extraordinary opportunity to explore this research topic.
</div>
<div class="Indented">
Second I have to make a pause to remember all the person that mattered most in my intelectual path, most of them are mentioned in my PhD preface <sup><a name="cite-6" href="#biblio-6" class="bibliocite">6</a></sup>. I would like to mention in particular Prof. Javier Ruiz-del-Solar who first introduced me in the topic of computer vision.
</div>
<div class="Indented">
Finally I would like to thanks my colleagues at INRIA, Mines Paris and KULeuven with whom I had and have many enriching discussions that have steered my way of thinking on the subject. I would like to mention in particular Nicolas Simond, Yann Dumortier, Julien Perret, and my current colleagues at VISICS as great positives influences.
</div>
<div class="Indented">

<span class="Newpage">

</span>
</div>
<div class="Indented">

<div class="fulltoc">

<div class="tocheader">
Table of Contents
</div><div class="tocindent">
<div class="tocindent">

<div class="toc">
<a href="#toc-Section*-1" class="Link">Section: What is this document ?</a>
</div>
<div class="toc">
<a href="#toc-Section*-2" class="Link">Section: License</a>
</div>
<div class="toc">
<a href="#toc-Section*-3" class="Link">Section: Acknowledgements</a>
</div></div>

<div class="toc">
<a href="#toc-Chapter-1" class="Link">Chapter 1: The big picture</a>
</div><div class="tocindent">

<div class="toc">
<a href="#toc-Section-1.1" class="Link">Section 1.1: The ultimate goal and the good enough goal</a>
</div>
<div class="toc">
<a href="#toc-Section-1.2" class="Link">Section 1.2: Buzz words</a>
</div></div>

<div class="toc">
<a href="#toc-Chapter-2" class="Link">Chapter 2: Limits and minimal assumptions</a>
</div><div class="tocindent">

<div class="toc">
<a href="#toc-Section-2.1" class="Link">Section 2.1: Monocular vision limits</a>
</div><div class="tocindent">

<div class="toc">
<a href="#toc-Subsection-2.1.1" class="Link">Subsection 2.1.1: Minimal assumptions for monocular vision</a>
</div></div>

<div class="toc">
<a href="#toc-Section-2.2" class="Link">Section 2.2: Stereo vision limits</a>
</div><div class="tocindent">

<div class="toc">
<a href="#toc-Subsection-2.2.1" class="Link">Subsection 2.2.1: Minimal assumptions for stereo vision</a>
</div></div>
</div>

<div class="toc">
<a href="#toc-Chapter-3" class="Link">Chapter 3: Traversability estimation</a>
</div><div class="tocindent">

<div class="toc">
<a href="#toc-Section-3.1" class="Link">Section 3.1: The map fallacy</a>
</div>
<div class="toc">
<a href="#toc-Section-3.2" class="Link">Section 3.2: Depth estimation from a stereo pair</a>
</div>
<div class="toc">
<a href="#toc-Section-3.3" class="Link">Section 3.3: Depth estimation from a stereo stream</a>
</div>
<div class="toc">
<a href="#toc-Section-3.4" class="Link">Section 3.4: Depth estimation from multiple images</a>
</div>
<div class="toc">
<a href="#toc-Section-3.5" class="Link">Section 3.5: Optical flow</a>
</div>
<div class="toc">
<a href="#toc-Section-3.6" class="Link">Section 3.6: Scene flow</a>
</div></div>

<div class="toc">
<a href="#toc-Chapter-4" class="Link">Chapter 4: Moving objects</a>
</div><div class="tocindent">

<div class="toc">
<a href="#toc-Section-4.1" class="Link">Section 4.1: Moving objects detection without motion estimation</a>
</div>
<div class="toc">
<a href="#toc-Section-4.2" class="Link">Section 4.2: The moving objects detection problem</a>
</div>
<div class="toc">
<a href="#toc-Section-4.3" class="Link">Section 4.3: Moving objects detection with motion estimation</a>
</div>
<div class="toc">
<a href="#toc-Section-4.4" class="Link">Section 4.4: Tracking</a>
</div></div>

<div class="toc">
<a href="#toc-Chapter-5" class="Link">Chapter 5: Mobile objects</a>
</div></div>

</div>
</div>
<h1 class="Chapter">
<a name="toc-Chapter-1" class="toc">1</a> The big picture
</h1>
<div class="Unindented">
Computer vision aims to extract information from images. This information is meant to support decision making. Which information should be extracted depends on which decisions should be done.
</div>
<div class="Indented">
In some cases we want to create accurate 3d models to analyse and reproduce a scene (e.g. building an elevation map from aerial images). Sometimes we want to count the presence of a certain element in the images (e.g. estimating the area of a forest). 
</div>
<div class="Indented">
In this text we will focus on the information required to allow a robot move safely on the streets of a city. We will assume that all the images have been taken from the moving robot itself and that the robot has access to a comercial grade navigation system (such as <a href="http://www.google.com/mobile/navigation/" class="URL">Google maps navigation</a>).
</div>
<h2 class="Section">
<a name="toc-Section-1.1" class="toc">1.1</a> The ultimate goal and the “good enough” goal
</h2>
<div class="Unindented">
It is commonly though that the ultimate goal of computer vision is “scene understanding”. To be able to see an image (or a set of images), and be able to extract all the information that is to be known about it: its 3d geometry, the objects presents, all the attribute of the objects, being able to predict the future behaviour of the objects and explain why such behaviour would occur. We would like machines to be able to understand images up to point where they can reason about it.
</div>
<div class="Indented">
Let us inspect the woman on figure <a href="#fig:woman-crossing" class="Reference">1.1↓</a>. From this picture a human can guess 
</div>
<div class="Indented">

<div class="float">
<a name="fig:woman-crossing" class="Label"> </a>
<div class="figure">

<div class="caption">
Figure 1.1 Woman crossing the street
</div>
</div>
</div>
</div>
<h2 class="Section">
<a name="toc-Section-1.2" class="toc">1.2</a> Buzz words
</h2>
<div class="Unindented">
&gt;&gt;&gt; If you arrived here you probably already heard many buzz words
</div>
<div class="Description">
<span class="Description-entry"> </span>given two images, optical flow tries to explain how did each pixel move from the first image to the second <sup><a name="cite-9" href="#biblio-9" class="bibliocite">9</a></sup>. It can be seen as a dense version of features tracking.
</div>
<div class="Description">
<span class="Description-entry"> </span>given two images and their known relative position stereo matching tries to find for every pixel in the first image the corresponding pixel in the second image <sup><a name="cite-3" href="#biblio-3" class="bibliocite">3</a>,<a name="cite-5" href="#biblio-5" class="bibliocite">5</a></sup>. Once the correspondence of a pixel is known then its 3d position can be estimated.
</div>
<div class="Description">
<span class="Description-entry"> </span>given a sequence of images, scene flow algorithms try to estimate the 3d position of each pixels and their velocity vector <sup><a name="cite-8" href="#biblio-8" class="bibliocite">8</a>,<a name="cite-1" href="#biblio-1" class="bibliocite">1</a></sup>. The projection of scene flow into 2d is equivalent to the optical flow.
</div>
<div class="Description">
<span class="Description-entry"> </span>given an image, image segmentation will try to group pixels by a given similarity criterion. The given criterion is application dependent, the grouping may be done by surface type, by appartenance to an object, or by any other relevant criterion. 
</div>
<div class="Description">
<span class="Description-entry"> </span>given a sequence of images, motion segmentation attempts to group pixel that have a similar 3d motion (rigid or non rigid). In order to estimate the 3d motion of each pixel point methods such as structure from motion and scene flow can be involved <sup><a name="cite-1" href="#biblio-1" class="bibliocite">1</a></sup>.
</div>
<div class="Description">
<span class="Description-entry"> </span>Given a set (or sequence) of images, a set of points appearing in multiple images is identified (tracks building via features detection and matching). Given the correspondences of the points in the images, structure from motion will attempt to reconstruct the 3d position of the points and of the cameras that took the images <sup><a name="cite-4" href="#biblio-4" class="bibliocite">4</a></sup>. 
</div>
<div class="Description">
<span class="Description-entry">SLAM </span>Simulateneous localization and mapping. If you havea sequence of observations and a map, then you can reconstruct your path on the map. If you have a sequence of observations and a know your path then you can build a map. SLAM attempts to do both simulateanously. This is today a mature problem in the robotics field, typically using laser scanners as sensors <sup><a name="cite-7" href="#biblio-7" class="bibliocite">7</a></sup>. In the computer vision field, the problem is known as Structure from motion (Sfm).
</div>
<div class="Description">
<span class="Description-entry"> </span>
</div>
<div class="Description">
<span class="Description-entry"> </span>
</div>
<div class="Description">
<span class="Description-entry"> </span>
</div>
<div class="Description">
<span class="Description-entry">Tracking </span>
</div>
<div class="Description">
<span class="Description-entry"> </span>
</div>
<div class="Description">
<span class="Description-entry"> </span>
</div>
<div class="Description">
<span class="Description-entry"> </span>
</div>
<div class="Description">
<span class="Description-entry">Context </span>
</div>
<h1 class="Chapter">
<a name="toc-Chapter-2" class="toc">2</a> <a name="cha:limits-and-assumptions" class="Label"> </a>Limits and minimal assumptions
</h1>
<h2 class="Section">
<a name="toc-Section-2.1" class="toc">2.1</a> Monocular vision limits
</h2>
<h3 class="Subsection">
<a name="toc-Subsection-2.1.1" class="toc">2.1.1</a> Minimal assumptions for monocular vision
</h3>
<h2 class="Section">
<a name="toc-Section-2.2" class="toc">2.2</a> Stereo vision limits
</h2>
<h3 class="Subsection">
<a name="toc-Subsection-2.2.1" class="toc">2.2.1</a> Minimal assumptions for stereo vision
</h3>
<h1 class="Chapter">
<a name="toc-Chapter-3" class="toc">3</a> Traversability estimation
</h1>
<h2 class="Section">
<a name="toc-Section-3.1" class="toc">3.1</a> The map fallacy
</h2>
<h2 class="Section">
<a name="toc-Section-3.2" class="toc">3.2</a> Depth estimation from a stereo pair
</h2>
<h2 class="Section">
<a name="toc-Section-3.3" class="toc">3.3</a> Depth estimation from a stereo stream
</h2>
<h2 class="Section">
<a name="toc-Section-3.4" class="toc">3.4</a> Depth estimation from multiple images
</h2>
<h2 class="Section">
<a name="toc-Section-3.5" class="toc">3.5</a> Optical flow
</h2>
<h2 class="Section">
<a name="toc-Section-3.6" class="toc">3.6</a> Scene flow
</h2>
<h1 class="Chapter">
<a name="toc-Chapter-4" class="toc">4</a> Moving objects
</h1>
<h2 class="Section">
<a name="toc-Section-4.1" class="toc">4.1</a> Moving objects detection without motion estimation
</h2>
<h2 class="Section">
<a name="toc-Section-4.2" class="toc">4.2</a> The moving objects detection problem
</h2>
<div class="Unindented">

</div>
<div class="Indented">
Let <span class="formula"><b>x</b><sub><i>fp</i></sub> = [<i>u</i>, <i>v</i>]<sup><i>T</i></sup> ∈ ℝ<sup>2</sup></span> be a point in image <span class="formula"><span class="script">I</span><sub><i>f</i></sub></span> corresponding to the projection of a point <span class="formula"><i>s</i><sub><i>p</i></sub> ∈ 𝔼<sup>3</sup></span> in the 3 dimensional euclidean space. We want to define a function <span class="formula"><i>f</i>(<b><b>x</b></b><sub><i>fp</i></sub>, <span class="script">I</span><sub>0..<i>f</i></sub>) ∈ {0, 1}</span> that will return <span class="formula">0</span> if <span class="formula"><i>s</i><sub><i>p</i></sub></span> had the same rigid motion than the ground plane in the frames <span class="formula">[0, <i>f</i>]</span> (static object) and <span class="formula">1</span> otherwise (moving object). 
</div>
<div class="Indented">
In order to make the problem slightly more tractable we will assume that the camera motion is smooth enough so pixels can be tracked from one frame to the next.
</div>
<div class="Indented">

</div>
<div class="Indented">
Moving objects detection based solely on the motion cue is a difficult task. When the robot moves every pixel in the image change, without knowing a priori the scene geometry nor the robot movement recognizing which pixels correspond to the static objects and which ones to moving objects is non trivial. Even when the both the scene geometry and the robot motion are know, the task still is non trivial since the motion maybe difficult to observe (small picture change), because moving objects are non rigid (no coherence between moving pixels), and because of appearance changes non related to movement such as shadows and reflections.
</div>
<h2 class="Section">
<a name="toc-Section-4.3" class="toc">4.3</a> Moving objects detection with motion estimation
</h2>
<h2 class="Section">
<a name="toc-Section-4.4" class="toc">4.4</a> Tracking
</h2>
<div class="Unindented">
Explain how tracking is different from optical flow (teddy bear turning over itself, teddy bear hides and comes back)
</div>
<h1 class="Chapter">
<a name="toc-Chapter-5" class="toc">5</a> Mobile objects
</h1>
<div class="Unindented">

<h1 class="biblio">
Bibliography
</h1>
<p class="biblio">
<span class="entry">[<a name="biblio-1" class="biblioentry">1</a>] </span> Andreas Wedel, <i>3D Motion Analysis via Energy Minimization</i>. , 2009.
</p>
<p class="biblio">
<span class="entry">[<a name="biblio-2" class="biblioentry">2</a>] </span> D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense two-frame stereo correspondence
	algorithms”, <i>International Journal of Computer Vision</i>, pp. 7—42, 2002.
</p>
<p class="biblio">
<span class="entry">[<a name="biblio-3" class="biblioentry">3</a>] </span> D. Scharstein and R. Szeliski, “A taxonomy and evaluation of dense two-frame stereo correspondence
	algorithms”, <i>International Journal of Computer Vision</i>, pp. 7-42, 2002.
</p>
<p class="biblio">
<span class="entry">[<a name="biblio-4" class="biblioentry">4</a>] </span> Ma, Yi and Soatto, Stefano and Kosecka, Jana and Sastry, Shankar
	S., <i>An Invitation to 3-D Vision</i>. Springer, 2003.
</p>
<p class="biblio">
<span class="entry">[<a name="biblio-5" class="biblioentry">5</a>] </span> Michael Bleyer, <i>Segmentation-based Stereo and Motion with Occlusions</i>. , 2006.
</p>
<p class="biblio">
<span class="entry">[<a name="biblio-6" class="biblioentry">6</a>] </span> Rodrigo Benenson, <i>Perception for driverless vehicles in urban environment</i>. , 2008.
</p>
<p class="biblio">
<span class="entry">[<a name="biblio-7" class="biblioentry">7</a>] </span> Sebastian Thrun and Wolfram Burgard and Dieter Fox, <i>Probabilistic Robotics (Intelligent Robotics and Autonomous Agents)</i>. MIT Press, 2005.
</p>
<p class="biblio">
<span class="entry">[<a name="biblio-8" class="biblioentry">8</a>] </span> Sundar Vedula and Simon Baker and Peter Rander and Robert Collins
	and Takeo Kanade, “Three-Dimensional Scene Flow”, <i></i>, pp. 722 - 729, 1999.
</p>
<p class="biblio">
<span class="entry">[<a name="biblio-9" class="biblioentry">9</a>] </span> Thomas Pock, <i>Fast Total Variation for Computer Vision</i>. , 2008.
</p>
</div>

<hr />
<p>Copyright (C) 2010 Created by </p>
</div>
<!-- Google analytics -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ?
"https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost +
"google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-80935-10");
pageTracker._trackPageview();
} catch(err) {}</script>
</body>
</html>
